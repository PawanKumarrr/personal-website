<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Comparative Neuroevolution - Pawan Kumar</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
            color: #333;
            background-color: #f4f4f4;
        }
        header {
            background: #2c3e50;
            color: #fff;
            text-align: center;
            padding: 2rem 0;
        }
        header h1 {
            margin: 0;
            font-size: 2.5rem;
        }
        nav {
            background: #34495e;
            padding: 1rem;
            position: sticky;
            top: 0;
            z-index: 100;
        }
        nav ul {
            list-style: none;
            padding: 0;
            margin: 0;
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
        }
        nav ul li {
            margin: 0 1rem;
        }
        nav ul li a {
            color: #fff;
            text-decoration: none;
            font-weight: bold;
            transition: color 0.3s;
        }
        nav ul li a:hover {
            color: #3498db;
        }
        section {
            padding: 2rem;
            max-width: 800px;
            margin: 0 auto;
            background: #fff;
            margin-bottom: 1rem;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        section h2 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 0.5rem;
            margin-bottom: 1rem;
        }
        ul {
            padding-left: 1.5rem;
        }
        ul li {
            margin-bottom: 0.5rem;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        footer {
            text-align: center;
            padding: 1rem;
            background: #2c3e50;
            color: #fff;
            position: relative;
            bottom: 0;
            width: 100%;
        }
        video {
            max-width: 100%;
            height: auto;
            margin: 1rem 0;
            border-radius: 8px;
        }
        .video-container {
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
            justify-content: center;
        }
        .video-item {
            flex: 1 1 45%;
            max-width: 45%;
            text-align: center;
        }
        .video-item p {
            margin: 0.5rem 0;
            font-size: 0.9rem;
            color: #2c3e50;
        }
        @media (max-width: 600px) {
            nav ul {
                flex-direction: column;
                align-items: center;
            }
            nav ul li {
                margin: 0.5rem 0;
            }
            header h1 {
                font-size: 1.8rem;
            }
            section {
                padding: 1rem;
            }
            .video-item {
                flex: 1 1 100%;
                max-width: 100%;
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>Teaching Machines to Move: Comparing Two AI Learning Methods</h1>
        <p>Pawan Kumar</p>
    </header>
    <nav>
        <ul>
            <li><a href="index.html">Back to Home</a></li>
        </ul>
    </nav>
    <section>
        <h2>Introduction</h2>
        <p>Imagine teaching a robot to move like a human—whether it’s a robotic arm reaching for an object or a legged robot walking across a room. My project, "Teaching Machines to Move," explores two ways to train artificial intelligence (AI) to control such movements in a simulated world called MuJoCo, where robots tackle tasks like reaching, walking, running, or balancing. The two methods are <strong>Proximal Policy Optimization (PPO)</strong>, which learns by fine-tuning its actions step-by-step, like practicing a skill with feedback, and <strong>Covariance Matrix Adaptation Evolution Strategy (CMA-ES)</strong>, which mimics evolution by testing many solutions and keeping the best ones, like nature selecting the strongest traits. By comparing these methods, I discovered how each excels in different scenarios, offering ideas for building smarter robots in fields like manufacturing, healthcare, or self-driving vehicles.</p>
        <p><a href="https://github.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo">Explore the project on GitHub</a></p>
    </section>
    <section>
        <h2>Why This Matters</h2>
        <p>The goal was to find out which method teaches robots better and faster, much like choosing the best way to learn a new sport. I tested PPO and CMA-ES on four tasks in MuJoCo:</p>
        <ul>
            <li><strong>Reacher</strong>: A robotic arm trying to touch a moving target, like playing a game of tag.</li>
            <li><strong>Ant</strong>: A four-legged robot learning to walk, similar to a puppy finding its balance.</li>
            <li><strong>HalfCheetah</strong>: A robot running forward, like a sprinter racing down a track.</li>
            <li><strong>InvertedDoublePendulum</strong>: A pole on a cart staying upright, like balancing a broom on your finger.</li>
        </ul>
        <p>I wanted to see how quickly each method learns (like mastering a skill) and how well the robots perform (like winning a game). This could help create robots that adapt to real-world challenges, such as navigating a factory or assisting in surgeries.</p>
    </section>
    <section>
        <h2>What If: A New Way to Train Robots?</h2>
        <p>What if we combined the strengths of PPO and CMA-ES to create robots that learn quickly <em>and</em> adapt to unexpected changes, like a delivery drone dodging obstacles in a storm? PPO is great at learning fast when the path is clear, like following a recipe. CMA-ES is better at exploring new possibilities, like experimenting with ingredients when the recipe fails. A hybrid approach could train robots to handle complex, unpredictable environments—think of self-driving cars navigating busy streets or robotic arms assembling products in a dynamic factory. This could spark new AI startups focused on adaptive robotics, transforming industries with machines that learn smarter and faster.</p>
    </section>
    <section>
        <h2>How It Worked</h2>
        <h3>The Testing Ground</h3>
        <p>MuJoCo is a virtual playground where robots practice tasks. Each task has specific challenges:</p>
        <ul>
            <li><strong>Reacher</strong>: The arm adjusts two joints to chase a target in a 2D space, with longer practice sessions to perfect its aim.</li>
            <li><strong>Ant</strong>: The robot coordinates eight joints to walk steadily in 3D, balancing speed and stability.</li>
            <li><strong>HalfCheetah</strong>: The robot uses six joints to run fast, rewarded for speed but penalized for wasteful moves.</li>
            <li><strong>InvertedDoublePendulum</strong>: The cart adjusts one force to keep a double pole upright, a tricky balancing act.</li>
        </ul>
        <h3>Training the AI</h3>
        <p>I used two approaches to train the AI:</p>
        <ul>
            <li><strong>PPO</strong>: Built with Stable-Baselines3, PPO learns by tweaking a neural network (like a brain) based on feedback, using settings like a learning speed of 0.0003 and practicing in 4–12 virtual worlds at once. It’s like a coach giving precise tips after each try.</li>
            <li><strong>CMA-ES</strong>: Created with DEAP, CMA-ES tests many “robot brains” at once, keeping the best performers and tweaking others, like breeding stronger plants. I used groups of 10–50 robots and ran up to 3000 rounds of tests.</li>
        </ul>
        <p>Both methods ran on a powerful computer cluster, ensuring fair and repeatable results. I tracked their progress with data logs and videos to see how the robots moved.</p>
        <h3>Measuring Success</h3>
        <p>I judged the methods by:</p>
        <ul>
            <li><strong>Score</strong>: How much reward the robot earned, like points in a game, averaged over 15–20 test runs.</li>
            <li><strong>Learning Speed</strong>: How fast the robot improved, shown in graphs.</li>
            <li><strong>Consistency</strong>: How steady the results were, using box plots to show score ranges.</li>
            <li><strong>Videos</strong>: Visual proof of the robots’ skills, like watching a performance.</li>
        </ul>
    </section>
    <section>
        <h2>What I Found</h2>
        <p>Each method shone in different ways across the tasks:</p>
        <ul>
            <li><strong>Reacher</strong>:
                <ul>
                    <li><strong>PPO</strong>: Learned quickly, scoring around -5 (close to the target), like hitting the bullseye reliably.</li>
                    <li><strong>CMA-ES</strong>: Scored around -15, improving slowly but exploring widely, like trying many paths to find a good one. See graphs at <a href="https://github.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo/blob/main/figures/cma_es_runs_separate.png">GitHub</a>.</li>
                </ul>
            </li>
            <li><strong>Ant</strong>:
                <ul>
                    <li><strong>PPO</strong>: Reached a score of 2000 fast, walking steadily, like a pro athlete.</li>
                    <li><strong>CMA-ES</strong>: Scored near 1800 after more time, with varied results, like a team still practicing. See box plot at <a href="https://github.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo/blob/main/figures/antBox.png">GitHub</a>.</li>
                </ul>
            </li>
            <li><strong>HalfCheetah</strong>:
                <ul>
                    <li><strong>PPO</strong>: Zoomed to 5500–6000, running smoothly, like a champion runner.</li>
                    <li><strong>CMA-ES</strong>: Hit 2500–3000, solid but less speedy, like a strong jogger. See comparison at <a href="https://github.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo/blob/main/figures/ppo_vs_cma_es_different_x_axes.png">GitHub</a>.</li>
                </ul>
            </li>
            <li><strong>InvertedDoublePendulum</strong>:
                <ul>
                    <li><strong>PPO</strong>: Mastered balancing at 9000, steady as a tightrope walker.</li>
                    <li><strong>CMA-ES</strong>: Reached 6000, good but less consistent, like balancing with some wobbles. See success at <a href="https://github.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo/blob/main/figures/vizz.png">GitHub</a>.</li>
                </ul>
            </li>
        </ul>
        <h3>Watch the Robots in Action</h3>
        <p>These videos show PPO-trained robots performing each task smoothly:</p>
        <div class="video-container">
            <div class="video-item">
                <video controls>
                    <source src="https://raw.githubusercontent.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo/main/videos/PPO/reacher.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <p>Reacher (PPO)</p>
            </div>
            <div class="video-item">
                <video controls>
                    <source src="https://raw.githubusercontent.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo/main/videos/PPO/Ant.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <p>Ant (PPO)</p>
            </div>
            <div class="video-item">
                <video controls>
                    <source src="https://raw.githubusercontent.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo/main/videos/PPO/HalfCheetha.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <p>HalfCheetah (PPO)</p>
            </div>
            <div class="video-item">
                <video controls>
                    <source src="https://raw.githubusercontent.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo/main/videos/PPO/IDP.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <p>InvertedDoublePendulum (PPO)</p>
            </div>
        </div>
        <p>CMA-ES videos are available in the <a href="https://github.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo/tree/main/videos/cma_es">GitHub repository</a>.</p>
        <h3>Key Lessons</h3>
        <ul>
            <li><strong>PPO</strong>: Learns fast and performs best when tasks have clear feedback, like following a map. It needed fewer tries (e.g., 2.4 million steps for Reacher) to excel.</li>
            <li><strong>CMA-ES</strong>: Explores broadly, great for tricky tasks where feedback is unclear, like finding a path in fog. It took more tries (e.g., 830 million steps for Reacher) but could handle surprises better.</li>
            <li><strong>Teamwork Potential</strong>: Combining PPO’s speed with CMA-ES’s exploration could create super-smart robots, like a student who studies fast but also thinks outside the box.</li>
        </ul>
    </section>
    <section>
        <h2>Try It Yourself</h2>
        <p>Anyone can recreate this project with:</p>
        <ul>
            <li><strong>Code</strong>: Python scripts and notebooks on <a href="https://github.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo">GitHub</a>.</li>
            <li><strong>Tools</strong>: Listed in <code>requirements.txt</code>, including MuJoCo, Stable-Baselines3, and DEAP.</li>
            <li><strong>Settings</strong>: Files for CMA-ES and PPO configurations.</li>
            <li><strong>Results</strong>: Data logs and graphs for scores and visuals.</li>
            <li><strong>Guide</strong>: Step-by-step instructions in the <a href="https://github.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo/blob/main/README.md">README</a>.</li>
        </ul>
        <p>Run it with these commands:</p>
        <pre><code>git clone https://github.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo.git
cd PPO-vs-CMAES-MuJoCo
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cd code/ppo
jupyter notebook</code></pre>
    </section>
    <section>
        <h2>What's Next?</h2>
        <p>This project opens doors to exciting possibilities:</p>
        <ul>
            <li><strong>Smarter Hybrids</strong>: Mix PPO and CMA-ES to build robots that learn fast and adapt to surprises, like a self-driving car handling sudden road changes.</li>
            <li><strong>Teamwork</strong>: Train multiple robots to work together, like a swarm of drones delivering packages.</li>
            <li><strong>Real-World Impact</strong>: Use these methods for robots in factories, hospitals, or space exploration.</li>
            <li><strong>Tough Tests</strong>: Check if CMA-ES handles messy feedback better than PPO, like navigating a noisy sensor.</li>
        </ul>
    </section>
    <section>
        <h2>Wrapping Up</h2>
        <p>My project shows how two AI methods—PPO and CMA-ES—teach robots to move in different ways. PPO is like a quick learner with clear instructions, while CMA-ES is a creative explorer finding new paths. By sharing my code and results, I hope to inspire others to build better AI for robots that can help in everyday life, from assembling products to exploring distant planets.</p>
    </section>
    <section>
        <p><em>Released under the MIT License. Videos, code, and figures are available on <a href="https://github.com/PawanKumarrr/PPO-vs-CMAES-MuJoCo">GitHub</a>.</em></p>
    </section>
    <footer>
        <p>© 2025 Pawan Kumar. All rights reserved.</p>
    </footer>
</body>
</html>